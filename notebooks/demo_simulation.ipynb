{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Network Simulation Demo\n",
    "\n",
    "This notebook demonstrates the basic functionality of the social network simulation framework using reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from src.environment.social_network_env import SocialNetworkEnv\n",
    "from src.agents.dqn_agent import DQNAgent\n",
    "from src.training.train import train_network\n",
    "from src.visualization.network_visualizer import NetworkVisualizer\n",
    "from src.visualization.propagation_graphs import PropagationVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create environment\n",
    "env_config = {\n",
    "    'num_agents': 50,\n",
    "    'initial_connections': 3,\n",
    "    'max_connections': 10,\n",
    "    'personality_dim': 5\n",
    "}\n",
    "env = SocialNetworkEnv(**env_config)\n",
    "\n",
    "# Create DQN agent\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "\n",
    "# Initialize visualizers\n",
    "network_vis = NetworkVisualizer()\n",
    "prop_vis = PropagationVisualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Initial Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get initial network state\n",
    "initial_network = env.network\n",
    "\n",
    "# Visualize initial network\n",
    "network_vis.visualize_network(\n",
    "    initial_network,\n",
    "    title=\"Initial Network Structure\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training parameters\n",
    "training_params = {\n",
    "    'num_episodes': 500,\n",
    "    'max_steps_per_episode': 100,\n",
    "    'eval_frequency': 50\n",
    "}\n",
    "\n",
    "# Train the network\n",
    "training_stats = train_network(env, agent, **training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(training_stats['episode_rewards'])\n",
    "plt.title('Training Rewards over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot network metrics\n",
    "metrics = training_stats['network_metrics']\n",
    "plt.figure(figsize=(12, 6))\n",
    "for key in metrics[0].keys():\n",
    "    values = [m[key] for m in metrics]\n",
    "    plt.plot(values, label=key)\n",
    "plt.title('Network Metrics Evolution')\n",
    "plt.xlabel('Evaluation Step')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ]
}
